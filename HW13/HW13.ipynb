{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMdXnO7reyXS66Cc5sx5Vtf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NolanRink/CS4540/blob/main/HW13/HW13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Ideas of WaveNet\n",
        "\n",
        "- **Autoregressive Raw-Waveform Model**  \n",
        "  WaveNet models an audio clip as a product of conditional probabilities:  \n",
        "  p(x) = p(x1) * p(x2 | x1) * p(x3 | x1, x2) * ... * p(xT | x1, ..., xT-1)  \n",
        "  It predicts each 8-bit µ-law sample one at a time, bypassing hand-engineered vocoders and learning the full speech production process directly from data.\n",
        "\n",
        "\n",
        "- **Dilated Causal Convolutions Enable Large Receptive Fields Without Recurrence**  \n",
        "  Causal convolutions ensure the generation order remains correct, while exponentially increasing dilation factors (1, 2, 4, … 512) allow the network to access hundreds of milliseconds of past context with just 30–40 layers, maintaining GPU efficiency.\n",
        "\n",
        "\n",
        "- **Gated Activation Units + Residual & Skip Connections for Deep Networks**  \n",
        "  The model uses gated activation units (tanh multiplied by sigmoid) inside residual blocks for stability and faster convergence. Skip connections funnel multi-scale information directly to the output softmax.\n",
        "\n",
        "\n",
        "- **Softmax Over µ-Law–Companded Values for Flexible Waveform Modeling**  \n",
        "  The model quantizes 16-bit audio to 256 µ-law bins, allowing tractable categorical distribution modeling that captures waveform detail better than Gaussian mixtures.\n",
        "\n",
        "\n",
        "- **Flexible Conditioning Mechanisms**  \n",
        "  - *Global conditioning*: Adds speaker ID or other global features as biases across all layers.  \n",
        "  - *Local conditioning*: Upsamples slower control streams (e.g., phonemes, log-F0, music tags) and injects them at each time step.  \n",
        "  This enables dynamic voice, language, or instrument switching.\n",
        "\n",
        "- **State-of-the-Art Naturalness for Text-to-Speech (TTS)**  \n",
        "  WaveNet achieved over 4.2 out of 5 Mean Opinion Scores (MOS) in US English and Mandarin, reducing the naturalness gap to real speech by over 50% compared to LSTM or unit-selection baselines.\n",
        "\n",
        "\n",
        "- **Versatile Beyond Speech**  \n",
        "  WaveNet can synthesize music fragments, perform raw-waveform phoneme recognition, and act as a discriminative model (e.g., achieving 18.8% phoneme error rate on the TIMIT dataset).\n",
        "\n",
        "\n",
        "- **Real-World Deployment & Impact**  \n",
        "  Google and DeepMind integrated WaveNet into Google Assistant and other products, providing natural-sounding voices and restoring personalized voices for people with speech impairments.\n",
        "\n",
        "\n",
        "- **Speed Bottleneck & Successor Models**  \n",
        "  Original WaveNet’s sample-by-sample generation was slow (about 20–30 times slower than real-time on 2016 GPUs). Successor models like Parallel WaveNet, WaveRNN, and Efficient Neural Audio Synthesis synthesize audio faster (up to 4 times real-time on a CPU) while maintaining quality.\n",
        "\n",
        "\n",
        "- **Key Architectural Lessons**  \n",
        "  1. Large receptive fields are essential for coherent audio.  \n",
        "  2. Convolutional autoregressive stacks scale better and train faster than RNNs for long sequences.  \n",
        "  3. Minimal signal-processing assumptions (only µ-law companding) allow the network to capture non-Gaussian audio details.  \n",
        "  4. Conditioning streams make WaveNet a flexible universal audio engine for TTS, music, enhancement, and recognition.\n",
        "\n",
        "\n",
        "- **Continuing Evolution (2024–2025)**  \n",
        "  Recent advancements include larger context models, diffusion-based decoders, and multimodal alignment, further closing the gap between synthetic and human speech.\n"
      ],
      "metadata": {
        "id": "oRt60W5HBl0b"
      }
    }
  ]
}