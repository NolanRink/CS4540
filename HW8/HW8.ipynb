{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NolanRink/CS4540/blob/main/HW8/HW8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gNDKtmOnu85"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/NolanRink/CS4540/blob/main/HW8/HW8.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XtuF0Zmgb43"
      },
      "source": [
        "#1.\n",
        "# Modern ways of studying the brain\n",
        "This video  provides a quick overview of methods researchers use to understand the brain. The video divides the methods into two categories, structural and functional. For structural it mentions CAT scans, which use X-rays to detect things like tumors or swelling, and MRIs, which give clearer pictures by using magnetic fields and radio waves. For brain function, the video explains EEGs which are a non-invasive technique measuring electrical activity from the scalp, helpful for detecting sleep stages or seizures. It also talks about MEG, which picks up the brain’s magnetic fields, providing better detail but needing more complicated equipment. The video highlights techniques combining both structure and function, especially fMRI, which shows active brain regions by tracking blood flow, and PET scans, which use radioactive glucose to map activity. The video mentions fMRI is more common because PET scans require injections, making them invasive. In summary, the video covers the basics, emphasizing what each method is good for and pointing out their limitations.\n",
        "\n",
        "# 2-Minute Neuroscience: Electroencephalography (EEG)\n",
        "This video explains EEGs which are a method used to measure electrical brain activity. It shows how EEGs doesn't record individual neurons but instead captures signals from groups of neurons firing together, usually around each electrode placed on the scalp. EEGs creates wave patterns with different frequencies and shapes, useful for monitoring brain activity during specific tasks or spontaneously. Clinically, EEGs helps diagnose conditions like epilepsy, track seizures, and assess sleep disorders. While EEGs are good for seeing general brain activity, it struggles with pinpointing exact brain locations because it gathers signals from large neuron groups, mainly on the brain's surface, making exact localization challenging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEpO4V7Xim3P"
      },
      "source": [
        "# 2.\n",
        "# Lecture 13 Time Series Analysis\n",
        "The video works to explain time series analysis in a pretty straightforward way. It starts by defining time series as data points collected at regular time intervals. The video emphasizes that a key difference between time series and just random samples (like a Monte Carlo simulation) is that time series often have trends, seasonality, and something called \"autocorrelation\" or \"memory. \"The instructor describes how time series can have changing means and standard deviations over time, using examples like CO2 levels and Nasdaq returns. CO2 levels show a clear upward trend with regular seasonal patterns, while Nasdaq returns show constant average but changing volatility. The video then compares regression and time series modeling. Regression uses independent variables to predict something, but time series modeling identifies patterns like trends and seasonality directly from the data without having to explicitly include those factors. The lecture discusses techniques for \"filtering\" data, like removing trends or seasonal patterns to better identify meaningful signals versus random noise. \"Noise\" refers to random variations, while \"signals\" represent actual underlying patterns. Another part of the video explains \"autocorrelation,\" which measures how much current values in a series are influenced by past values. High autocorrelation means values close in time affect each other strongly. The video also touches on \"smoothing,\" a method to filter out noise while keeping important trends clear. Finally, it discusses \"stationarity\" which means that the statistical properties like mean and variance stay constant over time, and how making data stationary helps simplify analysis.\n",
        "\n",
        "# Time Series ARIMA Models\n",
        "This video explains how to use ARIMA (AutoRegressive Integrated Moving Average) models in time series analysis. The video starts by describing basic terms like white noise, autoregressive (AR), and moving average (MA) processes. White noise is random with constant mean (zero) and variance, which is an ideal starting point for modeling. Next, it covers AR models, where the current value depends on previous values. It shows that AR models with positive parameters tend to keep moving in the same direction, while negative parameters cause oscillations. MA models, instead, depend on previous errors or residuals. Similar to AR, positive MA parameters continue trends, while negative parameters create oscillations. Combining these two gives ARMA models, blending past values and past errors. ARIMA models include differencing, used to stabilize data by removing trends making it stationary, which means there is a constant mean and variance. The video explains stationarity and how to achieve it through removing trends with regression orsubtracting consecutive data points. It also introduces the Dickey-Fuller test, a method to statistically check if data is stationary. The augmented Dickey-Fuller test adds more lags or trends to improve accuracy. ACF (autocorrelation function) and PACF (partial autocorrelation function) help identify AR and MA terms. AR models have slowly declining ACF and sharp cutoff PACF, while MA models show the opposite. ARMA models usually have both functions tailing off slowly. The video also explains how to detect and handle seasonalityF. For diagnostics, it suggests using the Ljung-Box test for white noise residuals and the AIC and BIC criteria for model selection, focusing on choosing simpler models that still fit well. The Box-Jenkins methodology summarizes steps for ARIMA modeling: identification (plotting data and checking ACF/PACF), estimation (fitting model parameters), and diagnostics (checking residuals). Overall, the video clearly covers key steps and considerations for ARIMA modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcbYL3qVml1d"
      },
      "source": [
        "# 3.\n",
        "# Fourier Transforms\n",
        "The Fourier transform is basically a math tool that lets us take a complicated signal like sounds, images, or any data that changes over time, and break it down into simpler parts made of sin waves. These sin waves are easier to analyze because each one is simple and has a clear frequency, amplitude, and phase. The main idea behind Fourier transforms, according to these videos is that any signal, even if it's complicated or messy can be created by adding up a bunch of simple sin waves. When these waves have the same frequency, adding them together just gives another sin wave with a different amplitude or phase. But adding waves of different frequencies creates more complicated patterns. Visually, the Fourier transform is like taking a signal graph and \"wrapping\" it around a circle at different speeds. When the wrapping speed matches the frequency of the signal, you see a clear pattern or spike, which means that frequency is strongly present. If the frequencies don’t match, the patterns balance out, showing little or no spike. The videos also mention practical examples, like audio editing, if you have a recording with some annoying background noise, you can use Fourier transforms to find and remove just that frequency. After changing things in the frequency domain, you can convert the signal back into the original domain using the inverse Fourier transform. Overall, the Fourier transform lets us switch between looking at a signal over time and looking at which frequencies are inside it, making it easier to analyze signals and solve problems in fields like engineering, physics, and audio processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvHbBQZTm7hs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPrrgbGHLbrd0LwNhWmrnVk",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
