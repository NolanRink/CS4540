{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNh7hF77jjF3h589DrBbSed",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NolanRink/CS4540/blob/main/Project3/Project3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sleep State Classification with FFNNs in PyTorch\n",
        "\n",
        "This notebook classifies local field potential (LFP) signals into **NREM** and **WAKE** states using feedforward neural networks (FFNNs) in PyTorch. We process data for three subjects, extract features, train multiple networks with different complexities, and visualize the results and performance.\n"
      ],
      "metadata": {
        "id": "rn95_FA3eT_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "Load the LFP data for three subjects from the following files (upload them manually to Colab):\n",
        "- \"Rat08-20130711_017.h5\"\n",
        "- \"Part1SubjectHB10.h5\"\n",
        "- \"Part2SubjectHB13.h5\""
      ],
      "metadata": {
        "id": "hH7oPIeDegFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# File paths for the three subjects\n",
        "files = [\n",
        "    \"Rat08-20130711_017.h5\",\n",
        "    \"Part1SubjectHB10.h5\",\n",
        "    \"Part2SubjectHB13.h5\"\n",
        "]\n",
        "\n",
        "# Define subject names corresponding to each file\n",
        "subject_names = [\"Rat08_HPC\", \"SubjectHB10_BLA\", \"SubjectHB13_BLA\"]\n",
        "\n",
        "data_dict = {}\n",
        "\n",
        "for subject, filename in zip(subject_names, files):\n",
        "    segments, labels = [], []\n",
        "    with h5py.File(filename, 'r') as f:\n",
        "        fs = f.attrs.get('fs', 1250)\n",
        "        for state in f.keys():\n",
        "            lab = 1 if state == \"NREM\" else 0\n",
        "            for seg in f[state].values():\n",
        "                segments.append(seg[()].astype(float))\n",
        "                labels.append(lab)\n",
        "    data_dict[subject] = {\"segments\": segments, \"labels\": np.array(labels), \"fs\": fs}\n",
        "    print(f\"{subject}: Loaded {len(segments)} segments\")\n"
      ],
      "metadata": {
        "id": "34lC2WunTvbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing\n",
        "\n",
        "Split each segment into 5-second clips and label them (0 for WAKE, 1 for NREM).\n",
        "\n"
      ],
      "metadata": {
        "id": "kRB2gUEOfMEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_duration = 5  # seconds\n",
        "default_fs = 1250\n",
        "processed_data = {}\n",
        "\n",
        "for subject in subject_names:\n",
        "    fs = data_dict[subject][\"fs\"] if \"fs\" in data_dict[subject] else default_fs\n",
        "    clip_length = int(clip_duration * fs)\n",
        "    X_clips = []\n",
        "    y_clips = []\n",
        "\n",
        "    segs = data_dict[subject][\"segments\"]\n",
        "    labs = data_dict[subject][\"labels\"]\n",
        "    for seg, lab in zip(segs, labs):\n",
        "        if len(seg) < clip_length:\n",
        "            continue\n",
        "        n_clips = len(seg) // clip_length\n",
        "        # Only use full clips; discard extra samples\n",
        "        seg_clips = seg[:n_clips * clip_length].reshape(n_clips, clip_length)\n",
        "        for clip in seg_clips:\n",
        "            X_clips.append(clip)\n",
        "        y_clips.extend([lab] * n_clips)\n",
        "\n",
        "    X_clips = np.array(X_clips)\n",
        "    y_clips = np.array(y_clips)\n",
        "    print(f\"{subject}: total {len(X_clips)} clips ({sum(y_clips==1)} NREM, {sum(y_clips==0)} WAKE).\")\n",
        "    processed_data[subject] = {\"X\": X_clips, \"y\": y_clips, \"fs\": fs}\n"
      ],
      "metadata": {
        "id": "vI9Fa0_vTyVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Feature Extraction\n",
        "\n",
        "For each 5-second clip, extract the following features:\n",
        "- Time domain: mean, standard deviation, skewness, kurtosis.\n",
        "- Frequency domain: Band power features in delta, theta, alpha, beta, and gamma bands using Welch’s PSD."
      ],
      "metadata": {
        "id": "UIzR9fF9fSe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal import welch\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "# Define frequency bands (in Hz)\n",
        "bands = {\n",
        "    \"delta\": (0.5, 4),\n",
        "    \"theta\": (4, 8),\n",
        "    \"alpha\": (8, 13),\n",
        "    \"beta\":  (13, 30),\n",
        "    \"gamma\": (30, 100)\n",
        "}\n",
        "\n",
        "for subject in subject_names:\n",
        "    X_clips = processed_data[subject][\"X\"]\n",
        "    fs = processed_data[subject][\"fs\"]\n",
        "    features = []\n",
        "    for clip in X_clips:\n",
        "        # Time-domain features\n",
        "        mu = np.mean(clip)\n",
        "        sigma = np.std(clip)\n",
        "        sk_val = skew(clip)\n",
        "        ku_val = kurtosis(clip)\n",
        "        # Frequency-domain: compute PSD with Welch’s method\n",
        "        freqs, psd = welch(clip, fs, nperseg=256)\n",
        "        band_powers = []\n",
        "        for low, high in bands.values():\n",
        "            mask = (freqs >= low) & (freqs < high)\n",
        "            power = np.trapz(psd[mask], freqs[mask]) if np.any(mask) else 0\n",
        "            band_powers.append(power)\n",
        "        features.append([mu, sigma, sk_val, ku_val] + band_powers)\n",
        "    features = np.array(features)\n",
        "    processed_data[subject][\"features\"] = features\n",
        "    print(f\"{subject}: Extracted features shape = {features.shape}\")\n"
      ],
      "metadata": {
        "id": "hNbCQDexT96M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Exploratory Visualization\n",
        "\n",
        "Visualize the PSD and feature distributions for Subject \"Rat08_HPC\" as an example.\n"
      ],
      "metadata": {
        "id": "4DWX5KUBfcFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Example: PSD plot for one NREM clip vs. one WAKE clip from \"Rat08_HPC\"\n",
        "fs = processed_data[\"Rat08_HPC\"][\"fs\"]\n",
        "# Assuming the first clip is NREM and the last clip is WAKE (based on original ordering)\n",
        "example_nrem_clip = processed_data[\"Rat08_HPC\"][\"X\"][0]\n",
        "example_wake_clip = processed_data[\"Rat08_HPC\"][\"X\"][-1]\n",
        "freqs_n, psd_n = welch(example_nrem_clip, fs, nperseg=256)\n",
        "freqs_w, psd_w = welch(example_wake_clip, fs, nperseg=256)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.semilogy(freqs_n, psd_n, label='NREM clip PSD')\n",
        "plt.semilogy(freqs_w, psd_w, label='WAKE clip PSD')\n",
        "plt.xlim(0, 50)\n",
        "plt.xlabel('Frequency (Hz)')\n",
        "plt.ylabel('PSD')\n",
        "plt.title('PSD of 5-second Clips (NREM vs. WAKE)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rKxvG19kT_54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Data Splitting and Normalization\n",
        "\n",
        "Split the data into training (70%), validation (15%), and test (15%) sets. Normalize features using z-score normalization based on the training set.\n"
      ],
      "metadata": {
        "id": "FKTOEpk1feLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "splits = {}\n",
        "\n",
        "for subject in subject_names:\n",
        "    X = processed_data[subject][\"features\"]\n",
        "    y = processed_data[subject][\"y\"]\n",
        "    # Split: Train+Val and Test\n",
        "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "        X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "    # Further split Train+Val into Train and Validation\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val, test_size=0.15/0.85, random_state=42, stratify=y_train_val)\n",
        "    # Normalize using training set statistics only\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train_sc = scaler.transform(X_train)\n",
        "    X_val_sc   = scaler.transform(X_val)\n",
        "    X_test_sc  = scaler.transform(X_test)\n",
        "    splits[subject] = {\n",
        "        \"X_train\": X_train_sc, \"y_train\": y_train,\n",
        "        \"X_val\": X_val_sc,     \"y_val\": y_val,\n",
        "        \"X_test\": X_test_sc,   \"y_test\": y_test\n",
        "    }\n",
        "    print(f\"{subject}: Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)} clips\")\n"
      ],
      "metadata": {
        "id": "N_nEVUgIUKTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model Definitions: Simple and Complex FFNNs\n",
        "\n",
        "Define two FFNN architectures:\n",
        "- **SimpleFFNN:** One hidden layer.\n",
        "- **ComplexFFNN:** Two hidden layers.\n"
      ],
      "metadata": {
        "id": "9xOBk2Msfkm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Simple FFNN with one hidden layer\n",
        "class SimpleFFNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=16, output_dim=2):\n",
        "        super(SimpleFFNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Complex FFNN with two hidden layers\n",
        "class ComplexFFNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims=[32, 16], output_dim=2):\n",
        "        super(ComplexFFNN, self).__init__()\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        for h in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            prev_dim = h\n",
        "        layers.append(nn.Linear(prev_dim, output_dim))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n"
      ],
      "metadata": {
        "id": "XnVaPFBcUMb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training the Models\n",
        "\n",
        "Train both models (SimpleFFNN and ComplexFFNN) on the training data for each subject. We monitor training and validation accuracy across epochs.\n",
        "\n",
        "The training function takes the data, trains the model for a specified number of epochs, and returns the accuracy history.\n",
        "\n"
      ],
      "metadata": {
        "id": "iJWsDr3PfqVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_ffnn(model, X_train, y_train, X_val, y_val, epochs=50, lr=0.01):\n",
        "    # Convert arrays to PyTorch tensors\n",
        "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "    X_val_t   = torch.tensor(X_val, dtype=torch.float32)\n",
        "    y_val_t   = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    history = {\"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train_t)\n",
        "        loss = criterion(outputs, y_train_t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            _, pred_train = torch.max(outputs, 1)\n",
        "            _, pred_val = torch.max(model(X_val_t), 1)\n",
        "            train_acc = (pred_train == y_train_t).float().mean().item()\n",
        "            val_acc = (pred_val == y_val_t).float().mean().item()\n",
        "\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
        "            print(f\"Epoch {epoch:2d}/{epochs}: Train Acc = {train_acc*100:.1f}%, Val Acc = {val_acc*100:.1f}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# Train models for each subject\n",
        "histories = {}  # Store training histories for plotting\n",
        "models = {}     # Store the trained models\n",
        "\n",
        "for subject in subject_names:\n",
        "    input_dim = splits[subject][\"X_train\"].shape[1]\n",
        "    print(f\"\\nSubject {subject} - Training Simple FFNN\")\n",
        "    model_simple = SimpleFFNN(input_dim=input_dim, hidden_dim=16, output_dim=2)\n",
        "    hist_simple = train_ffnn(model_simple,\n",
        "                             splits[subject][\"X_train\"], splits[subject][\"y_train\"],\n",
        "                             splits[subject][\"X_val\"], splits[subject][\"y_val\"],\n",
        "                             epochs=50, lr=0.01)\n",
        "\n",
        "    print(f\"\\nSubject {subject} - Training Complex FFNN\")\n",
        "    model_complex = ComplexFFNN(input_dim=input_dim, hidden_dims=[32, 16], output_dim=2)\n",
        "    hist_complex = train_ffnn(model_complex,\n",
        "                              splits[subject][\"X_train\"], splits[subject][\"y_train\"],\n",
        "                              splits[subject][\"X_val\"], splits[subject][\"y_val\"],\n",
        "                              epochs=50, lr=0.01)\n",
        "\n",
        "    histories[subject] = {\"simple\": hist_simple, \"complex\": hist_complex}\n",
        "    models[subject] = {\"simple\": model_simple, \"complex\": model_complex}\n"
      ],
      "metadata": {
        "id": "r5WztVGVUNr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(histories[\"Rat08_HPC\"][\"simple\"][\"train_acc\"], label=\"Simple Train\")\n",
        "plt.plot(histories[\"Rat08_HPC\"][\"simple\"][\"val_acc\"], label=\"Simple Val\")\n",
        "plt.plot(histories[\"Rat08_HPC\"][\"complex\"][\"train_acc\"], '--', label=\"Complex Train\")\n",
        "plt.plot(histories[\"Rat08_HPC\"][\"complex\"][\"val_acc\"], '--', label=\"Complex Val\")\n",
        "plt.title(\"Training/Validation Accuracy (Rat08_HPC)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eUtrrWD4WG2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Evaluating the Models\n",
        "\n",
        "Evaluate both models on the test data for each subject. Print the test accuracy and display the confusion matrix.\n"
      ],
      "metadata": {
        "id": "YhmL0i38fuYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "for subject in subject_names:\n",
        "    print(f\"\\nEvaluation for {subject}:\")\n",
        "    X_test = torch.tensor(splits[subject][\"X_test\"], dtype=torch.float32)\n",
        "    y_test = splits[subject][\"y_test\"]\n",
        "    for model_type in [\"simple\", \"complex\"]:\n",
        "        model = models[subject][model_type]\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(X_test)\n",
        "            _, y_pred = torch.max(outputs, 1)\n",
        "            y_pred = y_pred.numpy()\n",
        "        acc = (y_pred == y_test).mean()\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "\n",
        "        # Create the subplot for the confusion matrix.\n",
        "        fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
        "        ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
        "\n",
        "        # Annotate each cell with its numerical value.\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center', size='xx-large')\n",
        "\n",
        "        plt.xlabel('Predictions', fontsize=18)\n",
        "        plt.ylabel('Actuals', fontsize=18)\n",
        "        plt.title(f\"{model_type.capitalize()} FFNN Confusion Matrix\\nTest Accuracy: {acc*100:.2f}%\", fontsize=18)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "JEup4QyvUQLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(3, 2, figsize=(8, 10))\n",
        "for i, subject in enumerate(subject_names):\n",
        "    y_test = splits[subject][\"y_test\"]\n",
        "    for j, model_type in enumerate([\"simple\", \"complex\"]):\n",
        "        model = models[subject][model_type]\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            _, y_pred = torch.max(model(torch.tensor(splits[subject][\"X_test\"], dtype=torch.float32)), 1)\n",
        "            y_pred = y_pred.numpy()\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", cbar=False,\n",
        "                    ax=axes[i, j], xticklabels=[\"WAKE\", \"NREM\"], yticklabels=[\"WAKE\", \"NREM\"])\n",
        "        axes[i, j].set_title(f\"{subject} - {model_type.capitalize()} FFNN\")\n",
        "        axes[i, j].set_xlabel(\"Predicted Label\")\n",
        "        axes[i, j].set_ylabel(\"True Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tAt7Wuj6UVCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Results Summary and Discussion\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "All three subjects achieved robust classification of sleep state using feedforward networks. Delta power proved highly discriminative for NREM, while higher-frequency power was prevalent during wakefulness. Both simple and complex FFNNs performed similarly, suggesting the extracted 9-dimensional features are sufficiently informative. Issues such as potential feature redundancy and class imbalance were mitigated through feature normalization and stratified data splitting."
      ],
      "metadata": {
        "id": "6MUb9zEWgLzK"
      }
    }
  ]
}