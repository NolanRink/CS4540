{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [Evaluating a learning algorithm](https://www.youtube.com/watch?v=RmqDevlTd0c&ab_channel=BaseAccountSNair)\n",
        "\n",
        "- This video discusses how to effectively evaluate a machine learning algorithm once it has been trained. While knowing a machine learning technique is important, effectively applying it in practice is crucial to avoid wasting time on methods that won't improve performance. Often, practitioners randomly select different strategies to enhance their models—like collecting more training data, adding or reducing features, or adjusting regularization parameters. However, these approaches can be time-consuming, sometimes taking months, without guaranteed improvements. The video emphasizes that it's important to strategically decide what to try next rather than randomly picking methods. To address this, the video introduces the concept of machine learning diagnostics, a set of practical tests or techniques designed to quickly identify which strategies are most likely to help. Diagnostics allow developers to efficiently assess why a model is underperforming and what specific steps could improve results. By using these techniques, it's possible to save considerable time and effort by ruling out unpromising approaches early."
      ],
      "metadata": {
        "id": "vVFC8n5EyU7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Evaluating a Hypothesis](https://www.youtube.com/watch?v=beMxE1G1cDs&ab_channel=BaseAccountSNair)\n",
        "\n",
        "- This video explains how to evaluate a hypothesis learned by a machine learning algorithm. Even if a hypothesis achieves very low training error, it might still fail to generalize well to new data. This is known as overfitting. To check whether a hypothesis generalizes well, the standard practice is to split the available dataset into two parts, a training set and a test set. Usually, about 70% of the data is used for training the algorithm, while the remaining 30% is reserved as a test set to evaluate performance. It's important to randomly shuffle the data before dividing it especially if the data has any particular order For regression problems, the test error is calculated using the average squared difference between the predicted values and the actual values on the test set. In classification problems, the test error can be calculated using the logistic regression cost function, or by using a simpler metric known as misclassification error. Misclassification error measures the fraction of test examples incorrectly labeled by the hypothesis, usually based on a threshold (like 0.5) that decides which category an example belongs to."
      ],
      "metadata": {
        "id": "Q9oaK6aOwsBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Model selection Test Sets](https://www.youtube.com/watch?v=vXP1lBYficU&ab_channel=BaseAccountSNair)\n",
        "\n",
        "- This videodiscusses the issue of model selection in machine learning specifically, choosing among different models or adjusting model parameters (like the degree of a polynomial or regularization strength). Selecting the right model by evaluating it directly on the test set is problematic because it can lead to overly optimistic performance estimates; the model is essentially tuned to the test data, losing its objectivity in predicting real-world performance. To address this, the concept of dividing data into three separate sets: training, validation (or cross-validation), and test sets. Typically, the data might be split with about 60% used for training, 20% for validation, and 20% reserved strictly for testing. The training set is used to learn parameters. The validation set (also known as the cross-validation set) is then used to select among different models or parameters. After choosing the best-performing model on the validation set, the test set—completely separate and untouched until now—is used to get a reliable, unbiased estimate of the model’s generalization error.\n"
      ],
      "metadata": {
        "id": "oOibMIckx-60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [DiagBiasVar](https://www.youtube.com/watch?v=adpwpWtrrL4&ab_channel=BaseAccountSNair)\n",
        "\n",
        "- This video talks about the problem of figuring out why a machine learning algorithm might be performing poorly. Generally, poor performance occurs because of either high bias (underfitting) or high variance (overfitting). Understanding whether your model is facing bias or variance issues is importannt because it guides you toward the most effective strategies to improve your algorithm. the video uses the example of polynomial regression to illustrate these concepts. If a model is too simple it typically has high bias meaning it won’t even fit the training data well which results in high errors on both the training and validation sets. On the other hand, a model that's too complex (high-degree polynomial) may fit training data extremely well—sometimes even perfectly—but it will perform poorly on unseen data which indicates a high variance problem. To determine whether your algorithm has high bias or high variance, you should compare the errors on your training set and cross-validation set. A high bias situation typically shows high error on both the training set and the cross-validation set, meaning the model isn't fitting even the training data well. A high variance scenario usually has a very low error on the training set but a significantly higher error on the cross-validation set."
      ],
      "metadata": {
        "id": "1udta0yqzNu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [RegularizationBiasVar](https://www.youtube.com/watch?v=liL-VTq-D9k&ab_channel=BaseAccountSNair)\n",
        "\n",
        "- This video explains how regularization impacts the issues of bias and variance in machine learning. Regularization is a method used to prevent overfitting by discouraging the learning algorithm from choosing overly complex models with large parameter values. The choice of the regularization parameter (lambda) plays a big role. if lambda is very large, it forces the model parameters toward zero, resulting in a very simple, underfitting model (high bias). On the other hand, if lambda is very small or zero, the model can become overly complex, fitting the training data too closely and failing to generalize to new data, leading to high variance (overfitting). To choose the best lambda value, the vieo recommends training multiple models with different lambda values and evaluating their performance on a separate cross-validation set. After training these models, you select the lambda value that results in the lowest error on the cross-validation set. Then, you use a separate test set to accurately measure how well your final model generalizes. The video also describes how training and cross-validation errors change as lambda varies. For small lambda values (minimal regularization), the training error is usually low, but the cross-validation error remains high, indicating overfitting. For large lambda values (strong regularization), both training and cross-validation errors are high, showing underfitting. Typically, an intermediate lambda value provides the best balance, minimizing the cross-validation error"
      ],
      "metadata": {
        "id": "A8lUtz3Z0ZJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [LearningCurves](https://www.youtube.com/watch?v=66ukDTmpUWg&ab_channel=BaseAccountSNair)\n",
        "\n",
        "- This video explains learning curves whuch are a tool to diagnose whether your machine learning model has high bias (underfitting) or high variance (overfitting). Learning curves plot training and cross-validation errors against the number of training examples. In high-bias situations, both errors remain high and adding more data doesn't help much. In contrast, with high variance, training error is low but cross-validation error is high; adding more data usually helps the model generalize better.\n"
      ],
      "metadata": {
        "id": "N9y51tgJ1bC4"
      }
    }
  ]
}